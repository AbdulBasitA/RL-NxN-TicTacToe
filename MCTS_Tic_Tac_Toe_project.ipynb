{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8oZXw0x172rQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Node & Monte Carlo Tree Search Classes\n",
        "\n"
      ],
      "metadata": {
        "id": "1mQMylEbAFZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node():\n",
        "  def __init__(self, board, parent_node):\n",
        "    self.board = board\n",
        "    self.is_terminal = self.board.win() or self.board.draw()\n",
        "    self.parent = parent_node\n",
        "    self.total_returns = 0\n",
        "    self.number_action_taken = 0\n",
        "    self.expanded = self.is_terminal\n",
        "    self.children_nodes = {}\n",
        "\n",
        "class MCTS():\n",
        "  def search(self, starting_state, num_iterations):\n",
        "    self.root = Node(starting_state, None)\n",
        "\n",
        "    for iter in range(num_iterations):\n",
        "      node = self.selection(self.root)\n",
        "      total_returns = self.simulation(node.board)\n",
        "      self.backpropagation(node, total_returns)\n",
        "    \n",
        "    try:\n",
        "      return self.get_max_action(self.root, 0)\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  def selection(self, node):\n",
        "    while not node.is_terminal:\n",
        "      if node.expanded:\n",
        "        node = self.get_max_action(node, 2)\n",
        "      else:\n",
        "        return self.expansion(node)\n",
        "      \n",
        "    return node\n",
        "  \n",
        "  def expansion(self, node):\n",
        "    actions = node.board.get_valid_actions()\n",
        "    for act in actions:\n",
        "      if str(act.board_positions) not in node.children_nodes:\n",
        "        new_node = Node(act, node)\n",
        "\n",
        "        node.children_nodes[str(act.board_positions)] = new_node\n",
        "\n",
        "        if len(actions) == len(node.children_nodes):\n",
        "          node.expanded = True\n",
        "        \n",
        "        return new_node\n",
        "  \n",
        "  def get_max_action(self, node, exploration_constant):\n",
        "    max_return = float('-inf')\n",
        "    max_actions = []\n",
        "\n",
        "    for child in node.children_nodes.values():\n",
        "      curr_player = None\n",
        "      if child.board.second_player == 'X': curr_player = 1\n",
        "      elif child.board.second_player == 'O': curr_player = -1\n",
        "\n",
        "      UCB_val = curr_player * child.total_returns/child.number_action_taken + np.sqrt((exploration_constant * np.log(node.number_action_taken))/child.number_action_taken)\n",
        "      if UCB_val > max_return:\n",
        "        max_return = UCB_val\n",
        "        max_actions = [child]\n",
        "      \n",
        "      elif UCB_val == max_return:\n",
        "        max_actions.append(child)\n",
        "\n",
        "    return random.choice(max_actions)\n",
        "\n",
        "  def simulation(self, board):\n",
        "    while not board.win():\n",
        "      try:\n",
        "        board = random.choice(board.get_valid_actions())\n",
        "      except:\n",
        "        return 0\n",
        "    \n",
        "    if board.second_player == 'X': return 1\n",
        "    if board.second_player == 'O': return -1\n",
        "  \n",
        "  def backpropagation(self, node, total_returns):\n",
        "    while node:\n",
        "      node.number_action_taken += 1\n",
        "      node.total_returns += total_returns\n",
        "      node = node.parent\n"
      ],
      "metadata": {
        "id": "bTNcZw45BjjD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tic-Tac-Toe Board Environment"
      ],
      "metadata": {
        "id": "4qK9CnS_3qQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToeBoardEnvironment():\n",
        "  def __init__(self, num_rows, num_cols, prev_board=None):\n",
        "    self.first_player = 'X'\n",
        "    self.second_player = 'O'\n",
        "    self.curr_player = self.first_player\n",
        "    self.empty = '*'\n",
        "    self.num_rows = num_rows\n",
        "    self.num_cols = num_cols\n",
        "    self.board_positions = {}\n",
        "      \n",
        "    self.initialize_board()\n",
        "\n",
        "    if prev_board:\n",
        "      self.__dict__ = deepcopy(prev_board.__dict__)\n",
        "\n",
        "  def reset(self):\n",
        "    new_board = TicTacToeBoardEnvironment(self.num_rows, self.num_cols)\n",
        "    terminated = False\n",
        "    return new_board, terminated\n",
        "\n",
        "  def print_board(self):\n",
        "    for row in range(self.num_rows):\n",
        "      for col in range(self.num_cols):\n",
        "        print(self.board_positions[row, col], end = \" \")\n",
        "      print()\n",
        "    \n",
        "    self.print_player()\n",
        "  \n",
        "  def print_player(self):\n",
        "    print(\"\\n\" + self.curr_player + \" please make a move...\\n\")\n",
        "\n",
        "  def get_valid_actions(self):\n",
        "    actions = []\n",
        "    for row in range(self.num_rows):\n",
        "      for col in range(self.num_cols):\n",
        "        if self.board_positions[row, col] == self.empty:\n",
        "          actions.append(self.choose_position(row,col)) \n",
        "    \n",
        "    return actions\n",
        "  \n",
        "  def get_valid_actions_(self):\n",
        "    actions = []\n",
        "    for row in range(self.num_rows):\n",
        "      for col in range(self.num_cols):\n",
        "        if self.board_positions[row, col] == self.empty:\n",
        "          actions.append((row,col)) \n",
        "    \n",
        "    return actions\n",
        "\n",
        "  def initialize_board(self):\n",
        "    for row in range(self.num_rows):\n",
        "      for col in range(self.num_cols):\n",
        "        self.board_positions[row, col] = self.empty\n",
        "  \n",
        "  def choose_position(self, row, col):\n",
        "    new_board = TicTacToeBoardEnvironment(self.num_rows, self.num_cols, prev_board=self)\n",
        "    new_board.board_positions[row, col] = self.first_player\n",
        "    new_board.first_player, new_board.second_player = new_board.second_player, new_board.first_player\n",
        "    new_board.curr_player = new_board.first_player\n",
        "\n",
        "    return new_board\n",
        "  \n",
        "  def draw(self):\n",
        "    for row in range(self.num_rows):\n",
        "      for col in range(self.num_cols):\n",
        "        if self.board_positions[row, col] == self.empty:\n",
        "          return False\n",
        "    return True\n",
        "  \n",
        "  def win(self):\n",
        "    for row in range(self.num_rows):\n",
        "      num_curr_player = []\n",
        "      for col in range(self.num_cols):\n",
        "        if self.board_positions[row, col] == self.second_player:\n",
        "          num_curr_player.append((row, col))\n",
        "        if len(num_curr_player) == self.num_rows:\n",
        "          return True\n",
        "          \n",
        "    for col in range(self.num_cols):\n",
        "      num_curr_player = []\n",
        "      for row in range(self.num_rows):\n",
        "        if self.board_positions[row, col] == self.second_player:\n",
        "          num_curr_player.append((row, col))\n",
        "        if len(num_curr_player) == self.num_rows:\n",
        "          return True\n",
        "\n",
        "    num_curr_player = []\n",
        "    for row in range(self.num_rows):\n",
        "      col = row\n",
        "      if self.board_positions[row, col] == self.second_player:\n",
        "        num_curr_player.append((row,col))\n",
        "      if len(num_curr_player) == self.num_rows:\n",
        "        return True\n",
        "\n",
        "    num_curr_player = []\n",
        "    for row in range(self.num_rows):\n",
        "      col = self.num_rows - row - 1\n",
        "      if self.board_positions[row, col] == self.second_player:\n",
        "        num_curr_player.append((row,col))\n",
        "      if len(num_curr_player) == self.num_rows:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "    \n",
        "  def terminal(self):\n",
        "    if self.win():\n",
        "      return 1 if self.second_player == 'X' else -1\n",
        "    \n",
        "    if self.draw():\n",
        "      return 0\n",
        "    \n",
        "    return None\n",
        "\n",
        "  def step(self, action):\n",
        "    row, col = action\n",
        "    self = self.choose_position(row, col)\n",
        "    terminal_ = self.terminal()\n",
        "    if terminal_ is not None:\n",
        "      if terminal_ == 1:\n",
        "        return TicTacToeBoardEnvironment(self.num_rows, self.num_cols), 100, True, \"win\"\n",
        "      if terminal_ == 0:\n",
        "        return TicTacToeBoardEnvironment(self.num_rows, self.num_cols), 0, True, \"draw\"\n",
        "    \n",
        "    rnd_idx = np.random.choice(len(self.get_valid_actions_()))\n",
        "    row,col = self.get_valid_actions_()[rnd_idx]\n",
        "    self = self.choose_position(row, col)\n",
        "\n",
        "    terminal_ = self.terminal()\n",
        "    if terminal_ is not None:\n",
        "      if terminal_ == -1:\n",
        "        return TicTacToeBoardEnvironment(self.num_rows, self.num_cols), -100, True, \"loss\"\n",
        "      if terminal_ == 0:\n",
        "        return TicTacToeBoardEnvironment(self.num_rows, self.num_cols), 0, True, \"draw\"\n",
        "\n",
        "    return self, -1, False, \"in-progress\"\n",
        "\n",
        "\n",
        "  def play_game_random(self, num_planning_steps):\n",
        "    #print(\"Playing nxn Tic-Tac-Toe\\n\")\n",
        "    #self.print_board()\n",
        "\n",
        "    mcts = MCTS()\n",
        "    while True:\n",
        "      rnd_idx = np.random.choice(len(self.get_valid_actions_()))\n",
        "      row,col = self.get_valid_actions_()[rnd_idx]\n",
        "      #random.randint(0, self.num_rows-1)\n",
        "      #col = random.randint(0, self.num_cols-1)\n",
        "\n",
        "      while self.board_positions[row, col] != self.empty:\n",
        "        rnd_idx = np.random.choice(len(self.get_valid_actions_()))\n",
        "        row,col = self.get_valid_actions_()[rnd_idx]\n",
        "\n",
        "      self = self.choose_position(row, col)\n",
        "\n",
        "      max_action = mcts.search(self, num_planning_steps)\n",
        "      try:\n",
        "        self = max_action.board\n",
        "      except:\n",
        "        pass\n",
        "      #self.print_board()\n",
        "\n",
        "      terminal_ = self.terminal()\n",
        "      if terminal_ is not None: return terminal_\n",
        "\n"
      ],
      "metadata": {
        "id": "f-2re3hsANfI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###100 Games of Tic-Tac-Toe - MCTS(25,50,100 planning-steps) vs Random Opponent[board-sizes 3x3-10x10]\n"
      ],
      "metadata": {
        "id": "R_k9n97at3tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mcts_vs_random_results(planning_steps):\n",
        "  results = {\"3x3\": (), \"4x4\": (), \"5x5\": (), \"6x6\": (), \"7x7\": (), \"8x8\": (), \"9x9\": (), \"10x10\": ()}\n",
        "\n",
        "  for board_size in range(3, 4):\n",
        "    board = TicTacToeBoardEnvironment(board_size,board_size)\n",
        "    wins_mcts, wins_random, draws = 0,0,0\n",
        "    for i in tqdm(range(100)):\n",
        "      res = board.play_game_random(planning_steps)\n",
        "      if res == -1:\n",
        "        wins_mcts += 1\n",
        "      if res == 1:\n",
        "        wins_random += 1\n",
        "      if res == 0:\n",
        "        draws += 1\n",
        "    results[str(board_size) + \"x\" + str(board_size)] = (wins_mcts, wins_random, draws)\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "5GHnNTautHQp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcts_vs_random_results_25_planning_steps = mcts_vs_random_results(25)"
      ],
      "metadata": {
        "id": "-zHv4GtosVWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcts_vs_random_results_50_planning_steps = mcts_vs_random_results(50)"
      ],
      "metadata": {
        "id": "efnpt56_txb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcts_vs_random_results_100_planning_steps = mcts_vs_random_results(100)"
      ],
      "metadata": {
        "id": "urdVYFalwKwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 Iterations\n",
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = mcts_vs_random_results_25_planning_steps\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('MCTS wins against Random Agent for Various NxN sizes of boards over 100 games(simulations=25)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h4C4keWaz37H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 50 Iterations\n",
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = mcts_vs_random_results_50_planning_steps\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('MCTS wins against Random Agent for Various NxN sizes of boards over 100 games(simulations=50)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gUFO45G9mRi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 100 Iterations\n",
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = mcts_vs_random_results_100_planning_steps\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('MCTS wins against Random Agent for Various NxN sizes of boards over 100 games(simulations=100)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1dQICXtappuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###100 Games of Tic-Tac-Toe - MCTS(25,50,100 planning-steps) vs Itself[board-sizes 3x3-10x10]"
      ],
      "metadata": {
        "id": "EbFW4tJ10A2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mcts_vs_mcts_results(planning_steps):\n",
        "  results = {\"3x3\": (), \"4x4\": (), \"5x5\": (), \"6x6\": (), \"7x7\": (), \"8x8\": (), \"9x9\": (), \"10x10\": ()}\n",
        "\n",
        "  for board_size in range(3, 7):\n",
        "    mcts = MCTS()\n",
        "    count_wins = 0\n",
        "    count_draws = 0\n",
        "\n",
        "    for i in tqdm(range(100)):\n",
        "      msg = \"\"\n",
        "      board = TicTacToeBoardEnvironment(board_size, board_size)\n",
        "      while msg != \"invalid\":\n",
        "        max_act = mcts.search(board, planning_steps)\n",
        "        try:\n",
        "          board = max_act.board\n",
        "        except: \n",
        "          msg = \"invalid\"\n",
        "        #board.print_board()\n",
        "        if board.win():\n",
        "          count_wins += 1\n",
        "          msg = \"invalid\"\n",
        "\n",
        "        if board.draw():\n",
        "          count_draws += 1\n",
        "          msg = \"invalid\"\n",
        "    results[str(board_size) + \"x\" + str(board_size)] = (count_wins, count_draws)\n",
        "  return results"
      ],
      "metadata": {
        "id": "q-W2N7ap-CxU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcts_vs_mcts_results_25_planning_steps = mcts_vs_mcts_results(25)"
      ],
      "metadata": {
        "id": "YKM-JxHr0PhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcts_vs_mcts_results_50_planning_steps = mcts_vs_mcts_results(50)"
      ],
      "metadata": {
        "id": "r_seSOLb13ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcts_vs_mcts_results_100_planning_steps = mcts_vs_mcts_results(100)"
      ],
      "metadata": {
        "id": "3V2pRZCK0Y7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 Iterations\n",
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = mcts_vs_mcts_results_25_planning_steps\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('MCTS wins against itself for Various NxN sizes of boards over 100 games(simulations=25)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CtGSla6JRadO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 50 Iterations\n",
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = mcts_vs_mcts_results_50_planning_steps\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('Agent wins against itself for Various NxN sizes of boards over 100 games(simulations=50)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lygv2NUhngph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 100 Iterations\n",
        "\n",
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = mcts_vs_mcts_results_100_planning_steps\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('Agent wins against itself for Various NxN sizes of boards over 100 games(simulations=100)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_YVhV-mKL8lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA & Q-learning \n",
        "on Tic-Tac-Toe"
      ],
      "metadata": {
        "id": "xIPKht0e4AZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def SARSA(env, alpha, gamma, epsilon, num_episodes, final_games_num):\n",
        "  def policy(Q_vals, state, epsilon):\n",
        "    rand_ = np.random.rand()\n",
        "    valid_actions = state.get_valid_actions_()\n",
        "    if rand_ > epsilon:\n",
        "      act = np.argmax([Q_vals[(state,action)] for action in range(len(valid_actions))])\n",
        "      return valid_actions[act]\n",
        "    act = np.random.choice(len(valid_actions))\n",
        "    return valid_actions[act]\n",
        "\n",
        "  Q_vals = defaultdict(float)\n",
        "  final_games = {\"win\": 0, \"loss\": 0, \"draw\": 0}\n",
        "\n",
        "  for ep in tqdm(range(num_episodes)):\n",
        "    state, _ = env.reset()\n",
        "    action = policy(Q_vals, state, epsilon)\n",
        "    flag = False\n",
        "    while True:\n",
        "      next_state, reward, done, status = state.step(action)\n",
        "      next_action = policy(Q_vals, next_state, epsilon)\n",
        "      Q_vals[(state,action)] = Q_vals[(state,action)] + alpha * (reward + gamma * Q_vals[(next_state,next_action)] - Q_vals[(state,action)])\n",
        "      state,action = next_state,next_action\n",
        "      if done:\n",
        "        break\n",
        "    if ep + final_games_num >= num_episodes:\n",
        "      final_games[status] += 1\n",
        "    \n",
        "  return final_games"
      ],
      "metadata": {
        "id": "UlnkAZZjBg_l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SARSA_vs_random_results():\n",
        "  results = {\"3x3\": (), \"4x4\": (), \"5x5\": (), \"6x6\": (), \"7x7\": (), \"8x8\": (), \"9x9\": (), \"10x10\": ()}\n",
        "  for board_size in range(3, 11):\n",
        "    final_games = SARSA(TicTacToeBoardEnvironment(board_size, board_size), 0.5, 0.9, 0.1, 1000, 100)\n",
        "    results[str(board_size) + \"x\" + str(board_size)] = final_games['win']\n",
        "  return results"
      ],
      "metadata": {
        "id": "BCLGHKI3wuVI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "spQiXRDcxmFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa_results = SARSA_vs_random_results()"
      ],
      "metadata": {
        "id": "2_2qlN4A_L6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = list(sarsa_results.values())\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('SARSA wins against Random Agent for Various NxN sizes of boards over 100 games(num_episodes=1000)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nx47iEf94Dv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8KXMS6H84H_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def Q_learning(env, alpha, gamma, epsilon, num_episodes, final_games_num):\n",
        "  def policy(Q_vals, state, epsilon):\n",
        "    rand_ = np.random.rand()\n",
        "    valid_actions = state.get_valid_actions_()\n",
        "    if rand_ > epsilon:\n",
        "      act = np.argmax([Q_vals[(state,action)] for action in range(len(valid_actions))])\n",
        "      return valid_actions[act]\n",
        "    act = np.random.choice(len(valid_actions))\n",
        "    return valid_actions[act]\n",
        "\n",
        "  Q_vals = defaultdict(float)\n",
        "  final_games = {\"win\": 0, \"loss\": 0, \"draw\": 0}\n",
        "\n",
        "  for ep in tqdm(range(num_episodes)):\n",
        "    state, _ = env.reset()\n",
        "    while True:\n",
        "      action = policy(Q_vals, state, epsilon)\n",
        "      next_state, reward, done, status = state.step(action)\n",
        "      next_state_valid_actions = next_state.get_valid_actions_()\n",
        "      Q_MAX = np.max([Q_vals[(next_state, act)] for act in next_state_valid_actions])\n",
        "      Q_vals[(state, action)] = Q_vals[(state, action)] + alpha * (reward + gamma * Q_MAX - Q_vals[(state, action)])\n",
        "      state = next_state\n",
        "      if done:\n",
        "        break\n",
        "    if ep + final_games_num >= num_episodes:\n",
        "      final_games[status] += 1\n",
        "\n",
        "  return final_games"
      ],
      "metadata": {
        "id": "jfwGckCynTmz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Q_learning_vs_random_results():\n",
        "  results = {\"3x3\": (), \"4x4\": (), \"5x5\": (), \"6x6\": (), \"7x7\": (), \"8x8\": (), \"9x9\": (), \"10x10\": ()}\n",
        "  for board_size in range(3, 11):\n",
        "    final_games = Q_learning(TicTacToeBoardEnvironment(board_size, board_size), 0.5, 0.9, 0.1, 1000, 100)\n",
        "    results[str(board_size) + \"x\" + str(board_size)] = final_games['win']\n",
        "  return results"
      ],
      "metadata": {
        "id": "-DZ4N20MoRKJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_learning_results = Q_learning_vs_random_results()"
      ],
      "metadata": {
        "id": "mAS3LLw6owt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects = ('3x3', '4x4', '5x5', '6x6', '7x7', '8x8', '9x9', '10x10')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = list(q_learning_results.values())\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Number of Wins')\n",
        "plt.ylabel('Board Size')\n",
        "\n",
        "plt.title('Q-learning wins against Random Agent for Various NxN sizes of boards over 100 games(num_episodes=1000)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ss22EKTOzAQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}